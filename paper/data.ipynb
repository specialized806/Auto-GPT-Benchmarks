{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21013 entries, 0 to 25470\n",
      "Data columns (total 19 columns):\n",
      " #   Column                Non-Null Count  Dtype              \n",
      "---  ------                --------------  -----              \n",
      " 0   createdAt             21013 non-null  datetime64[ns]     \n",
      " 1   agent                 21013 non-null  object             \n",
      " 2   costUSD               21013 non-null  float64            \n",
      " 3   job_id                21013 non-null  object             \n",
      " 4   challenge             21013 non-null  object             \n",
      " 5   benchmark_start_time  21013 non-null  datetime64[ns, UTC]\n",
      " 6   prompt                21013 non-null  object             \n",
      " 7   response              21013 non-null  object             \n",
      " 8   model                 21013 non-null  object             \n",
      " 9   request               21013 non-null  object             \n",
      " 10  attempted             15008 non-null  object             \n",
      " 11  categories            15008 non-null  object             \n",
      " 12  task                  15008 non-null  object             \n",
      " 13  success               14835 non-null  float64            \n",
      " 14  difficulty            15008 non-null  object             \n",
      " 15  success_%             14835 non-null  float64            \n",
      " 16  run_time              15008 non-null  object             \n",
      " 17  is_regression         15008 non-null  object             \n",
      " 18  report_time           15008 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](2), datetime64[ns](1), float64(3), object(13)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('../reports/df.pkl')\n",
    "df.dropna(subset=['benchmark_start_time', 'response', 'model', 'agent'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges_to_ignore = [\"TestBasicSafety_BuildNuke\",\"TestAgentProtocol_ExecuteAgentTaskStep\", \"TestAgentProtocol_GetAgentTask\", \"TestAgentProtocol_ListAgentTasksIds\", \"TestAgentProtocol_CreateAgentTask\"]\n",
    "agents_to_ignore = ['gpt-engineer', 'smol-developer', 'babyagi', 'evo', 'auto-gpt-turbo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def is_action_auto_gpt(log):\n",
    "    \"\"\"AutoGPTs actions are defined by the presence of the \"command\" key.\n",
    "\n",
    "    World state actions\n",
    "    - web_search\n",
    "    - write_to_file\n",
    "    - browse_website\n",
    "    - execute_python_file\n",
    "    - list_files\n",
    "    - execute_python_code\n",
    "    Internal actions\n",
    "    - goals_accomplished\n",
    "\n",
    "    Input\n",
    "        The \"content\" key of an LLM response\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for the existence of the \"command\" key in the log\n",
    "    command_existence = bool(re.search(r'\"command\"\\s*:', log))\n",
    "\n",
    "    if command_existence:\n",
    "        # Convert the JSON-like string to a Python dictionary\n",
    "        log_dict = json.loads(log)\n",
    "\n",
    "        # Check if the \"command\" key exists and has a \"name\" key\n",
    "        if \"command\" in log_dict and \"name\" in log_dict[\"command\"]:\n",
    "            command_name = log_dict[\"command\"][\"name\"]\n",
    "\n",
    "            # List of command names that signify an action\n",
    "            action_command_names = [\n",
    "                \"web_search\",\n",
    "                \"write_to_file\",\n",
    "                \"browse_website\",\n",
    "                \"execute_python_file\",\n",
    "                \"list_files\",\n",
    "                \"execute_python_code\",\n",
    "                \"read_file\",\n",
    "            ]\n",
    "\n",
    "            # Check if the command name matches any in the list\n",
    "            return command_name in action_command_names\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_openai_function(log):\n",
    "    \"\"\"OpenAI API function calls are determined by the presence of the \"function_call\" key.\n",
    "    Beebot\n",
    "    World state actions\n",
    "    - get_html_content\n",
    "    - read_file\n",
    "    - write_file\n",
    "    - wolfram_alpha_query\n",
    "    - write_python_code\n",
    "    - execute_python_file\n",
    "    - google_search\n",
    "    - wikipedia\n",
    "    - install_python_package\n",
    "    - execute_python_file_in_background\n",
    "    - get_process_status\n",
    "    - kill_process\n",
    "    - analyze_webpage_content\n",
    "    - get_website_text_content\n",
    "    - gmail_get_message\n",
    "    - gmail_create_draft\n",
    "    - disk_usage\n",
    "    Internal actions\n",
    "    - get_more_tools\n",
    "    - exit\n",
    "    - rewind_actions\n",
    "    - delegate_task\n",
    "    - function_summary\n",
    "\n",
    "    PolyGPT\n",
    "    World state actions\n",
    "    - http.\n",
    "    - filesystem.\n",
    "    - ethers.\n",
    "    - ipfs.\n",
    "    - web-scraper.\n",
    "    - ens.\n",
    "    - safe-factory.\n",
    "    - safe-manager.\n",
    "    Internal actions\n",
    "    - LearnWrap\n",
    "    - InvokeWrap\n",
    "    - user\n",
    "\n",
    "    Input\n",
    "        The entire LLM response\n",
    "    \"\"\"\n",
    "    # Check for the existence of the \"function_call\" key in the log\n",
    "    function_call_existence = bool(log.get(\"function_call\", None))\n",
    "\n",
    "    if function_call_existence:\n",
    "        # Check if the \"function_call\" key exists and has a \"name\" key\n",
    "        if \"name\" in log[\"function_call\"]:\n",
    "            function_name = log[\"function_call\"][\"name\"]\n",
    "\n",
    "            # List of function names that signify an action\n",
    "            action_function_names = [\n",
    "                \"read_file\",\n",
    "                \"write_\",\n",
    "                \"wolfram_alpha_query\",\n",
    "                \"execute_\",\n",
    "                \"install_python_package\",\n",
    "                \"get_\",\n",
    "                \"kill_process\",\n",
    "                \"encyclopedia\",\n",
    "                \"gmail_\",\n",
    "                \"disk_usage\",\n",
    "                \"os_name_and_version\",\n",
    "                \"analyze_webpage_content\",\n",
    "                \"google_\",\n",
    "                \"wikipedia\",\n",
    "                \"http.\",\n",
    "                \"filesystem.\",\n",
    "                \"ethers.\",\n",
    "                \"ipfs.\",\n",
    "                \"web-scraper.\",\n",
    "                \"ens.\",\n",
    "                \"safe-factory.\",\n",
    "                \"safe-manager.\",\n",
    "            ]\n",
    "\n",
    "            # Check if the function name matches any in the list\n",
    "            return any(function_name in action for action in action_function_names)\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_action_miniagi(log):\n",
    "    \"\"\"Mini-AGI function calls are determined by the presence of different patterns\n",
    "    World state actions\n",
    "    - execute_python\n",
    "    - web_search\n",
    "    - execute_shell\n",
    "    - ingest_data\n",
    "    - process_data\n",
    "    Internal actions\n",
    "    - done\n",
    "    - talk_to_user\n",
    "    - memorize_thoughts\n",
    "    \"\"\"\n",
    "    # List of function names that signify an action\n",
    "    action_function_names = [\n",
    "        \"execute_python\",\n",
    "        \"web_search\",\n",
    "        \"execute_shell\",\n",
    "        \"ingest_data\",\n",
    "        \"process_data\",\n",
    "    ]\n",
    "\n",
    "    # Check for the <c>...</c> pattern and whether it matches any action function names\n",
    "    c_pattern_match = False\n",
    "    c_pattern_search = re.search(r\"<c>(.*?)<\\/c>\", log)\n",
    "    if c_pattern_search:\n",
    "        c_pattern_match = c_pattern_search.group(1) in action_function_names\n",
    "\n",
    "    # Check for the \"ACTION:\" pattern and whether it matches any action function names\n",
    "    action_pattern_match = False\n",
    "    action_pattern_search = re.search(r\"ACTION:\\s*(\\w+)\\s*(\\(x\\d+\\))?\", log)\n",
    "    if action_pattern_search:\n",
    "        action_pattern_match = action_pattern_search.group(1) in action_function_names\n",
    "\n",
    "    return c_pattern_match or action_pattern_match\n",
    "\n",
    "\n",
    "def is_action_turbo(log):\n",
    "    \"\"\"Turbos actions are defined by the presence of the \"cmd\" key.\n",
    "    World state actions\n",
    "    - search\n",
    "    - www\n",
    "    - py\n",
    "    - aol\n",
    "    - put\n",
    "    - pyf\n",
    "    Internal actions\n",
    "    - end\n",
    "    \"\"\"\n",
    "    # List of function names that signify an action\n",
    "    action_function_names = [\"search\", \"www\", \"py\", \"aol\", \"put\", \"pyf\", \"sh\", \"ls\"]\n",
    "\n",
    "    # Check for the \"cmd\" key pattern and whether its \"name\" field matches any action function names\n",
    "    cmd_pattern_match = False\n",
    "    cmd_pattern_search = re.search(r'\"cmd\"\\s*:\\s*{\\s*\"name\"\\s*:\\s*\"(\\w+)\"', log)\n",
    "    if cmd_pattern_search:\n",
    "        cmd_pattern_match = cmd_pattern_search.group(1) in action_function_names\n",
    "\n",
    "    return cmd_pattern_match\n",
    "\n",
    "\n",
    "def is_action_general(log):\n",
    "    \"\"\"General actions are defined by the presence of specific keywords such as 'write', 'start', 'create', etc.\n",
    "    KEYWORDS FOUND SO FAR\n",
    "    WRITE\n",
    "    - write\n",
    "    - start\n",
    "    - create\n",
    "    - execute\n",
    "    - post\n",
    "    MODIFY\n",
    "    - modify\n",
    "    - mutate\n",
    "    - delete\n",
    "    - put\n",
    "    READ\n",
    "    - read\n",
    "    - list\n",
    "    - search\n",
    "    - find\n",
    "    - get\n",
    "    - browse\n",
    "    - query\n",
    "    - www\n",
    "    GENERAL, no specificity\n",
    "    - command\n",
    "    - call\n",
    "    - function\n",
    "    - action\n",
    "    - http\n",
    "    \"\"\"\n",
    "    if log is None:\n",
    "        return False\n",
    "\n",
    "    if log.get(\"content\", \"\"):\n",
    "        log = log[\"content\"]\n",
    "    elif log.get(\"function_call\", \"\"):\n",
    "        log = json.dumps(log[\"function_call\"])\n",
    "\n",
    "    if isinstance(log, dict):\n",
    "        print(\"log is dict\", log)\n",
    "\n",
    "    return bool(\n",
    "        re.search(\n",
    "            r\"\\b(write|start|create|execute|post|modify|mutate|delete|put|search|find|get|browse|query|www|read|list|http)\\b\",\n",
    "            log,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def is_action_agent(log, agent, test=\"\", response=\"\"):\n",
    "    \"\"\"Determines if a log contains an action based on patterns from different agents.\"\"\"\n",
    "    is_action = False\n",
    "\n",
    "    if log is None:\n",
    "        print(\"Log is None\", agent, test, response)\n",
    "        return is_action\n",
    "\n",
    "    log_content = log.get(\"content\", \"\")\n",
    "\n",
    "    if agent == \"auto-gpt\":\n",
    "        is_action = is_action_auto_gpt(log_content)\n",
    "    elif agent in [\"beebot\", \"polygpt\"]:\n",
    "        is_action = is_openai_function(log)\n",
    "    elif agent == \"miniagi\":\n",
    "        is_action = is_action_miniagi(log_content)\n",
    "    elif agent == \"turbo\":\n",
    "        is_action = is_action_turbo(log_content)\n",
    "\n",
    "    return is_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to convert JSON-like strings to nested dictionaries\n",
    "def nested_json(x):\n",
    "    if pd.notna(x):\n",
    "        d = json.loads(x)\n",
    "        if \"content\" in d and isinstance(d[\"content\"], str):\n",
    "            try:\n",
    "                d[\"content\"] = json.loads(d[\"content\"])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        return d\n",
    "    return x\n",
    "\n",
    "agent_array = set(df['agent'].unique()) - set(agents_to_ignore)\n",
    "challenges_array = set(df['challenge'].unique()) - set(challenges_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "challenge = \"TestRememberMultipleIds\"\n",
    "\n",
    "# Loop through unique agents\n",
    "for agent in agent_array:\n",
    "    \n",
    "    master_response_dict = OrderedDict()\n",
    "    master_response_nested_dict = OrderedDict()\n",
    "    master_actions_dict = OrderedDict()\n",
    "    master_request_dict = OrderedDict()\n",
    "    master_general_actions_dict = OrderedDict()\n",
    "    \n",
    "    # Replace with your actual DataFrame\n",
    "    selected_df = df.loc[(df['agent'] == agent) & (df['challenge'] == challenge)]\n",
    "\n",
    "    # Group by 'benchmark_start_time'\n",
    "    grouped_df = selected_df.groupby('benchmark_start_time')\n",
    "\n",
    "    for timestamp, group in grouped_df:\n",
    "        response_dict = OrderedDict()\n",
    "        response_nested_dict = OrderedDict()\n",
    "        actions_dict = OrderedDict()\n",
    "        request_dict = OrderedDict()\n",
    "        general_actions_dict = OrderedDict()\n",
    "        \n",
    "        total_rows = len(group)\n",
    "        \n",
    "        for i, (_, row) in enumerate(group.iterrows()):\n",
    "            response = json.loads(row['response'])\n",
    "            request = row['request']\n",
    "            response_nested = nested_json(row['response'])\n",
    "            \n",
    "            response_dict[str(total_rows-i)] = response\n",
    "            request_dict[str(total_rows-i)] = request\n",
    "            response_nested_dict[str(total_rows-i)] = response_nested\n",
    "            \n",
    "            if is_action_agent(response, agent, challenge, response):\n",
    "                actions_dict[str(total_rows-i)] = response\n",
    "\n",
    "            if is_action_general(response):\n",
    "                general_actions_dict[str(total_rows-i)] = response\n",
    "        \n",
    "        response_dict = OrderedDict(reversed(list(response_dict.items())))\n",
    "        response_nested_dict = OrderedDict(reversed(list(response_nested_dict.items())))\n",
    "        actions_dict = OrderedDict(reversed(list(actions_dict.items())))\n",
    "        request_dict = OrderedDict(reversed(list(request_dict.items())))\n",
    "        general_actions_dict = OrderedDict(reversed(list(general_actions_dict.items())))\n",
    "        \n",
    "        master_response_dict[str(timestamp)] = response_dict\n",
    "        master_response_nested_dict[str(timestamp)] = response_nested_dict\n",
    "        master_actions_dict[str(timestamp)] = actions_dict\n",
    "        master_request_dict[str(timestamp)] = request_dict\n",
    "        master_general_actions_dict[str(timestamp)] = general_actions_dict\n",
    "    \n",
    "    os.makedirs(f'specific/{challenge}', exist_ok=True)\n",
    "    os.makedirs(f'specific/{challenge}/{agent}', exist_ok=True)\n",
    "    \n",
    "    with open(f'specific/{challenge}/{agent}/response.json', 'w') as f:\n",
    "        json.dump(master_response_dict, f, indent=4)\n",
    "\n",
    "    with open(f'specific/{challenge}/{agent}/regex_specific.json', 'w') as f:\n",
    "        json.dump(master_actions_dict, f, indent=4)\n",
    "\n",
    "    with open(f'specific/{challenge}/{agent}/response_nested.json', 'w') as f:\n",
    "        json.dump(master_response_nested_dict, f, indent=4)\n",
    "        \n",
    "    with open(f'specific/{challenge}/{agent}/request.json', 'w') as f:\n",
    "        json.dump(master_request_dict, f, indent=4)\n",
    "\n",
    "    with open(f'specific/{challenge}/{agent}/regex_simple.json', 'w') as f:\n",
    "        json.dump(master_general_actions_dict, f, indent=4)\n",
    "        \n",
    "    with open(f'specific/{challenge}/{agent}/response_malicious.json', 'w') as f:\n",
    "        json.dump(master_response_nested_dict, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Initialize the master challenge dictionaries outside the agent loop\n",
    "master_challenge_response_dict = {}\n",
    "master_challenge_request_dict = {}\n",
    "\n",
    "# Loop through unique agents\n",
    "for agent in agent_array:\n",
    "    \n",
    "    # Initialize dictionaries to hold data per challenge\n",
    "    master_response_dict_per_challenge = {}\n",
    "    master_request_dict_per_challenge = {}\n",
    "\n",
    "    for challenge in challenges_array:\n",
    "        \n",
    "        master_response_dict = OrderedDict()\n",
    "        master_request_dict = OrderedDict()\n",
    "        \n",
    "        # Replace with your actual DataFrame\n",
    "        selected_df = df.loc[(df['agent'] == agent) & (df['challenge'] == challenge)]\n",
    "\n",
    "        # Group by 'benchmark_start_time'\n",
    "        grouped_df = selected_df.groupby('benchmark_start_time')\n",
    "\n",
    "        for timestamp, group in grouped_df:\n",
    "            response_dict = OrderedDict()\n",
    "            request_dict = OrderedDict()\n",
    "            \n",
    "            total_rows = len(group)\n",
    "            \n",
    "            for i, (_, row) in enumerate(group.iterrows()):\n",
    "                response = json.loads(row['response'])\n",
    "                request = row['request']\n",
    "                \n",
    "                response_dict[str(total_rows-i)] = response\n",
    "                request_dict[str(total_rows-i)] = request\n",
    "            \n",
    "            response_dict = OrderedDict(reversed(list(response_dict.items())))\n",
    "            request_dict = OrderedDict(reversed(list(request_dict.items())))\n",
    "            \n",
    "            master_response_dict[str(timestamp)] = response_dict\n",
    "            master_request_dict[str(timestamp)] = request_dict\n",
    "        \n",
    "        master_response_dict_per_challenge[challenge] = master_response_dict\n",
    "        master_request_dict_per_challenge[challenge] = master_request_dict\n",
    "\n",
    "    # Now write the JSON files, using the master dictionaries for each challenge\n",
    "    os.makedirs(f'all_logs/{agent}', exist_ok=True)\n",
    "\n",
    "    with open(f'all_logs/{agent}/response.json', 'w') as f:\n",
    "        json.dump(master_response_dict_per_challenge, f, indent=4)\n",
    "\n",
    "    with open(f'all_logs/{agent}/regex_specific.json', 'w') as f:\n",
    "        json.dump(master_actions_dict_per_challenge, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# List of agents to process\n",
    "for agent in agent_array:\n",
    "    # Read the existing 'response.json' file\n",
    "    with open(f'all_logs/{agent}/response.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Initialize dictionaries to store filtered logs\n",
    "    filtered_specific = {}\n",
    "    filtered_general = {}\n",
    "\n",
    "    for challenge, timestamps in data.items():\n",
    "        filtered_specific[challenge] = {}\n",
    "        filtered_general[challenge] = {}\n",
    "        \n",
    "        for timestamp, logs in timestamps.items():\n",
    "            filtered_specific[challenge][timestamp] = {}\n",
    "            filtered_general[challenge][timestamp] = {}\n",
    "            \n",
    "            for log_num, log in logs.items():\n",
    "                if is_action_agent(log, agent):\n",
    "                    filtered_specific[challenge][timestamp][log_num] = log\n",
    "                if is_action_general(log):\n",
    "                    filtered_general[challenge][timestamp][log_num] = log\n",
    "\n",
    "    # Save the filtered logs as new JSON files\n",
    "    with open(f'all_logs/{agent}/filtered_specific.json', 'w') as f:\n",
    "        json.dump(filtered_specific, f, indent=4)\n",
    "\n",
    "    with open(f'all_logs/{agent}/filtered_general.json', 'w') as f:\n",
    "        json.dump(filtered_general, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malicious_actions.json', 'r') as f:\n",
    "    malicious_logs = json.load(f)\n",
    "    \n",
    "if agent in ['auto-gpt', 'turbo']:\n",
    "    response_nested = nested_json(row['response'])\n",
    "\n",
    "filtered_array = [item for item in malicious_logs if agent in item[\"agent\"]]\n",
    "\n",
    "# if the command matches\n",
    "\n",
    "# Randomly choose one element from the filtered array\n",
    "if filtered_array:\n",
    "    random_element = random.choice(filtered_array)\n",
    "    print(\"Random element containing agent1:\", random_element)\n",
    "else:\n",
    "    print(f\"No elements contain {agent}.\")\n",
    "    \n",
    "\n",
    "with open(f'{agent}/response_malicious_inserted.json', 'w') as f:\n",
    "    json.dump(master_response_nested_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
